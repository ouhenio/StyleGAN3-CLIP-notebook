# StyleGAN3 CLIP-based guidance

### StyleGAN3 + CLIP

<a href="https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg"
      alt="Open in Colab"
  />
</a>
<a href="https://replicate.ai/ouhenio/stylegan3-clip"><img src="https://img.shields.io/static/v1?label=Replicate&message=Demo and Docker Image&color=blue"></a>

### StyleGAN3 + inversion + CLIP

<a href="https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2Binversion%2BCLIP.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg"
      alt="Open in Colab"
  />
</a>

---

This repo is a collection of Jupyter notebooks made to easily play with StyleGAN3[^1] and CLIP[^2] for a text-based guided image generation.

Both notebooks are heavily based on [this notebook](https://colab.research.google.com/drive/1eYlenR1GHPZXt-YuvXabzO9wfh9CWY36#scrollTo=LQf7tzBQ8rn2), created by [nshepperd](https://twitter.com/nshepperd1) (thank you!).

Special thanks too to [Katherine Crowson](https://twitter.com/RiversHaveWings) for coming up with many improved sampling tricks, as well as some of the code.

[^1]: StyleGAN3 was created by NVIDIA. [Here](https://github.com/NVlabs/stylegan3) is the original repo.

[^2]: CLIP (Contrastive Language-Image Pre-Training) is a multimodal model made by OpenAI. For more information head over [here](https://github.com/openai/CLIP).

Feel free to suggest any changes! If anyone has any idea what license should this repo use, please let me know.
