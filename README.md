# StyleGAN3 CLIP-based guidance

<a href="https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2BCLIP.ipynb">
<img src="https://colab.research.google.com/assets/colab-badge.svg"
     alt="Open in Colab"
/>
</a>
<a href="https://replicate.ai/ouhenio/stylegan3-clip"><img src="https://img.shields.io/static/v1?label=Replicate&message=Demo and Docker Image&color=blue"></a>

Edited version of [this notebook](https://colab.research.google.com/drive/1eYlenR1GHPZXt-YuvXabzO9wfh9CWY36#scrollTo=LQf7tzBQ8rn2), created by [nshepperd](https://twitter.com/nshepperd1)(https://github.com/nshepperd).

---

to-dos:

- Add inversion
- Add model mixins

---

This notebook uses work made by [Katherine Crowson](https://twitter.com/RiversHaveWings)(https://github.com/crowsonkb).

StyleGAN3 was created by NVIDIA. [Here](https://github.com/NVlabs/stylegan3) is the original repo.

CLIP (Contrastive Language-Image Pre-Training) is a model made by OpenAI. For more information head over [here](https://github.com/openai/CLIP).

Feel free to suggest any changes! If anyone has any idea what license should this repo use, please let me know.
